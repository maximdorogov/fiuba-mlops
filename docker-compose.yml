x-airflow-common:
  &airflow-common
  # image: apache/airflow:2.10.1
  build:
    context: .
    dockerfile: ./airflow/Dockerfile
  environment:
    &airflow-common-env
    PYTHONUNBUFFERED: 1
    AIRFLOW__LOGGING__LOGGING_LEVEL: WARNING
    AIRFLOW__CORE__LOGGING_LEVEL: WARNING
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__API__AUTH_BACKEND: airflow.api.auth.backend.basic_auth
    _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-amazon==9.2.0 boto3 pandas"
    AWS_ACCESS_KEY_ID: root
    AWS_SECRET_ACCESS_KEY: 12345678
    MINIO_ENDPOINT: http://s3:9000
    MINIO_BUCKET: csv-data
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    - postgres
    - s3

services:
  inference_api:
    restart: always
    depends_on:
      mlflow:
        condition: service_healthy
    build:
      context: .
      dockerfile: ./inference_api/Dockerfile
    environment:
      - AWS_ACCESS_KEY_ID=root  
      - AWS_SECRET_ACCESS_KEY=12345678
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    ports:
      - "8000:8000"

  mlflow:
    restart: always
    build:
      context: .
      dockerfile: ./mlflow/Dockerfile
    container_name: mlflow
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "5000:5000"
    environment:
      - AWS_ACCESS_KEY_ID=root
      - AWS_SECRET_ACCESS_KEY=12345678
      - MLFLOW_S3_ENDPOINT_URL=http://s3:9000
    command: >
      mlflow server 
      --backend-store-uri postgresql://airflow:airflow@postgres:5432/mlflow_db
      --host 0.0.0.0
      --default-artifact-root s3://mlflow/
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://mlflow:5000 || exit 1
      interval: 60s
      timeout: 10s
      retries: 3

  s3:
    restart: always
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9010:9000"
      - "9011:9001"
    environment:
      - MINIO_ROOT_USER=root
      - MINIO_ROOT_PASSWORD=12345678
    volumes:
      - minio_data:/data
    command: server /data --console-address :9001
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 60s
      timeout: 20s
      retries: 3

  postgres:
    restart: always
    build:
      context: .
      dockerfile: ./postgres/Dockerfile
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=airflow
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DATABASE=airflow
      - POSTGRES_PORT=5432
    volumes:
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "5432", "-U", "airflow"]
      interval: 60s
      timeout: 20s
      retries: 3

  webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\""]
      interval: 30s
      timeout: 10s
      retries: 3

  airflow-init:
    <<: *airflow-common
    command: >
      bash -c "airflow db migrate &&
               (airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || echo 'User already exists') &&
               (airflow connections add 'minio_conn' --conn-type 'aws' --conn-host 's3' --conn-port '9000' --conn-extra '{\"endpoint_url\": \"http://s3:9000\", \"aws_access_key_id\": \"root\", \"aws_secret_access_key\": \"12345678\"}' || echo 'Connection already exists') &&
               echo 'Airflow initialization completed successfully'"
    restart: no
    depends_on:
      postgres:
        condition: service_healthy
      s3:
        condition: service_healthy


  # runs once and exits to create the required S3 buckets
  create_s3_buckets:
    image: minio/mc:latest
    container_name: minio_create_bucket
    depends_on:
      - s3
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc alias set s3 http://s3:9000 root 12345678;
      /usr/bin/mc mb s3/mlflow;
      /usr/bin/mc mb s3/data;
      /usr/bin/mc mb s3/csv-data;
      /usr/bin/mc mb s3/csv-data/incoming;
      /usr/bin/mc mb s3/csv-data/processed;
      exit 0;
      '

networks:
  default:
    driver: bridge

volumes:
  db_data:
  minio_data: